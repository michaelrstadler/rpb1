{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import flymovie as fm\n",
    "import cnn_models\n",
    "import cnn_models.siamese_cnn as cn\n",
    "import cnn_models.evaluate_models as ev\n",
    "from flymovie.simnuc import Sim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from fpdf import FPDF\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import scipy.spatial\n",
    "import scipy.ndimage as ndi\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['AaW_8455_13.89_400_771_1_0.41_35_6.05_11.37_rep0.pkl', 'cio_21108_12.58_612_588_2_0.02_35_7.52_16.69_rep0.pkl', 'dXJ_22327_10.58_523_331_1_0.52_39_7.09_10.57_rep0.pkl', 'ciW_7535_13.44_277_215_1_0.55_40_6.86_26.96_rep0.pkl', 'Eer_24451_8.45_630_287_2_0.89_41_4.23_19.98_rep0.pkl', 'DyX_22492_11.67_104_195_2_0.87_49_9.04_17.88_rep0.pkl', 'cOp_16676_12.32_400_893_2_0.73_25_3.94_11.68_rep0.pkl', 'AoB_21007_8.06_739_671_1_0.23_42_1.28_13.87_rep0.pkl', 'DgW_35477_8.46_519_661_1_0.44_39_9.94_20.81_rep0.pkl', 'DoR_14494_11.88_705_617_1_0.72_24_8.68_22.75_rep0.pkl'], array([[8.4550e+03, 1.3890e+01, 4.0000e+02, 7.7100e+02, 1.0000e+00,\n",
      "        4.1000e-01, 3.5000e+01, 6.0500e+00, 1.1370e+01],\n",
      "       [2.1108e+04, 1.2580e+01, 6.1200e+02, 5.8800e+02, 2.0000e+00,\n",
      "        2.0000e-02, 3.5000e+01, 7.5200e+00, 1.6690e+01],\n",
      "       [2.2327e+04, 1.0580e+01, 5.2300e+02, 3.3100e+02, 1.0000e+00,\n",
      "        5.2000e-01, 3.9000e+01, 7.0900e+00, 1.0570e+01],\n",
      "       [7.5350e+03, 1.3440e+01, 2.7700e+02, 2.1500e+02, 1.0000e+00,\n",
      "        5.5000e-01, 4.0000e+01, 6.8600e+00, 2.6960e+01],\n",
      "       [2.4451e+04, 8.4500e+00, 6.3000e+02, 2.8700e+02, 2.0000e+00,\n",
      "        8.9000e-01, 4.1000e+01, 4.2300e+00, 1.9980e+01],\n",
      "       [2.2492e+04, 1.1670e+01, 1.0400e+02, 1.9500e+02, 2.0000e+00,\n",
      "        8.7000e-01, 4.9000e+01, 9.0400e+00, 1.7880e+01],\n",
      "       [1.6676e+04, 1.2320e+01, 4.0000e+02, 8.9300e+02, 2.0000e+00,\n",
      "        7.3000e-01, 2.5000e+01, 3.9400e+00, 1.1680e+01],\n",
      "       [2.1007e+04, 8.0600e+00, 7.3900e+02, 6.7100e+02, 1.0000e+00,\n",
      "        2.3000e-01, 4.2000e+01, 1.2800e+00, 1.3870e+01],\n",
      "       [3.5477e+04, 8.4600e+00, 5.1900e+02, 6.6100e+02, 1.0000e+00,\n",
      "        4.4000e-01, 3.9000e+01, 9.9400e+00, 2.0810e+01],\n",
      "       [1.4494e+04, 1.1880e+01, 7.0500e+02, 6.1700e+02, 1.0000e+00,\n",
      "        7.2000e-01, 2.4000e+01, 8.6800e+00, 2.2750e+01]]))\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "def make_im_param_inputs(folder, n_repeats=1, mip=False, batch_size=32):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def get_files_params(subfolder):\n",
    "        files = os.listdir(subfolder)\n",
    "        files = [x if x[0] != '.' else None for x in files]\n",
    "        n_params = len(files[0].split('_')) - 2\n",
    "        params = np.zeros((0, n_params))\n",
    "        for f in files:\n",
    "            p = f.split('_')[1:-1]\n",
    "            p = [float(x) for x in p]\n",
    "            p = np.expand_dims(np.array(p), axis=0)\n",
    "            params = np.vstack((params, p))\n",
    "        return files, params\n",
    "\n",
    "    def batch_fetch(ds, batch_size):\n",
    "        \"\"\"Optimize dataset for fast parallel retrieval.\"\"\"\n",
    "        ds = ds.batch(batch_size, drop_remainder=False)\n",
    "        ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    # Set up input directories.\n",
    "    cache_dir=folder\n",
    "    anchor_images_path = cache_dir / \"left\"\n",
    "    positive_images_path = cache_dir / \"right\"\n",
    "    \n",
    "    print(get_files_params(anchor_images_path))\n",
    "    # Create datasets from these sorted files. These are in order and match in pairs.\n",
    "    \"\"\"\n",
    "    anchor_dataset = tf.data.Dataset.from_tensor_slices(im_files)\n",
    "    \n",
    "    # Combine datasets to form triplet images, apply shuffle to the whole\n",
    "    # dataset. So upon iteration, order of triplets will be random, and\n",
    "    # within triplets, the negative image will shuffle.\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_full_size)\n",
    "    dataset = dataset.take(dataset_take_size)\n",
    "\n",
    "    # Apply preprocessing and rotation via special mappable functions.\n",
    "    if mip:\n",
    "        dataset = dataset.map(preprocess_triplets_mip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        if rotate:\n",
    "            dataset = dataset.map(random_rotate_triplets_mip, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if not mip:\n",
    "        dataset = dataset.map(preprocess_triplets_3d, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        if rotate:\n",
    "            dataset = dataset.map(random_rotate_triplets_3d, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Divide into training and evaluation, batch and prefetch.\n",
    "    train_dataset = dataset.take(round(dataset_take_size * 0.8))\n",
    "    val_dataset = dataset.skip(round(dataset_take_size * 0.8))\n",
    "\n",
    "    train_dataset = batch_fetch(train_dataset, batch_size)\n",
    "    val_dataset = batch_fetch(val_dataset, batch_size)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "    \"\"\"\n",
    "make_im_param_inputs(Path('/Users/michaelstadler/Bioinformatics/Projects/rpb1/results/testsims_uPoNivMJ_10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "a = np.array(a)\n",
    "a = np.expand_dims(a, axis=(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.ndarray((0,4))\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e372816e4ed8f4776be7691c42762f9e24664f8c81a636e19b7b5da78d6a9ebf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
