{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import flymovie as fm\n",
    "import cnn_models\n",
    "import cnn_models.siamese_cnn as cn\n",
    "import cnn_models.evaluate_models as ev\n",
    "from flymovie.simnuc import Sim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from fpdf import FPDF\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import scipy.spatial\n",
    "import scipy.ndimage as ndi\n",
    "import tensorflow as tf\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_models.siamese_cnn import preprocess_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_models.siamese_cnn import preprocess_image\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "#---------------------------------------------------------------------------\n",
    "def get_target_shape(dir_):\n",
    "    \"\"\"Retrieve the image target size.\"\"\"\n",
    "    left = dir_ / 'left'\n",
    "    imfile = os.listdir(left)[-1]\n",
    "    impath = os.path.join(left, imfile)\n",
    "    with open(impath, 'rb') as file:\n",
    "        im = pickle.load(file)\n",
    "\n",
    "    shape = im.shape\n",
    "    return shape\n",
    "\n",
    "def make_im_param_inputs(folder, batch_size=32):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def get_files_params(subfolder):\n",
    "        files = os.listdir(subfolder)\n",
    "        files = [x if x[0] != '.' else None for x in files]\n",
    "        n_params = len(files[0].split('_')) - 2\n",
    "        params = np.zeros((0, n_params))\n",
    "        for f in files:\n",
    "            p = f.split('_')[1:-1]\n",
    "            p = [float(x) for x in p]\n",
    "            p = np.expand_dims(np.array(p), axis=0)\n",
    "            params = np.vstack((params, p))\n",
    "        files = [os.path.join(subfolder, x) for x in files]\n",
    "        return files, list(params)\n",
    "\n",
    "    def preprocess(file, paramset):\n",
    "        [im,] = tf.py_function(preprocess_image,[file,],[tf.float32,])\n",
    "        return im, paramset\n",
    "\n",
    "    # Set up input directories.\n",
    "    cache_dir=folder\n",
    "    left_path = cache_dir / \"left\"\n",
    "    right_path = cache_dir / \"right\"\n",
    "    \n",
    "    files_l, params_l = get_files_params(left_path)\n",
    "    files_r, params_r = get_files_params(right_path)\n",
    "    files = files_l + files_r\n",
    "    params = params_l + params_r\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((files, params))\n",
    "    dataset = dataset.shuffle(buffer_size=len(files))\n",
    "\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "    # Create datasets from these sorted files. These are in order and match in pairs.\n",
    "    \n",
    "ds = make_im_param_inputs(Path('/Users/michaelstadler/Bioinformatics/Projects/rpb1/results/testsims_uPoNivMJ_10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = get_target_shape(cache_dir)\n",
    "base_cnn = cn.make_base_cnn_3d(target_shape, 'base_cnn', nlayers=8)\n",
    "\n",
    "flatten = layers.Flatten()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(9)(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(base_cnn.input, output, name=\"Model\")\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=[\"acc\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 - 12s - loss: 3737.2253 - acc: 0.0500\n",
      "Epoch 2/5\n",
      "1/1 - 9s - loss: 11794.1465 - acc: 0.2000\n",
      "Epoch 3/5\n",
      "1/1 - 9s - loss: 6266.0391 - acc: 0.3500\n",
      "Epoch 4/5\n",
      "1/1 - 9s - loss: 6394.3345 - acc: 0.3500\n",
      "Epoch 5/5\n",
      "1/1 - 9s - loss: 5369.0830 - acc: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x184c4e850>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    ds,\n",
    "    epochs=5,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.layers.merge.Multiply"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching triplets...\n",
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "train, val = cn.make_triplet_inputs(Path('/Users/michaelstadler/Bioinformatics/Projects/rpb1/results/testsims_uPoNivMJ'), batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 34, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in val:\n",
    "    print(i[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.random((20,10))\n",
    "scaled = (a - a.mean(axis=0)) / a.std(axis=0)\n",
    "len(list(scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.randint(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "nfiles = 10\n",
    "filenames = []\n",
    "for _ in range(nfiles):\n",
    "    filename = ''.join([random.choice(string.ascii_letters) for i in range(3)])\n",
    "    for i in range(8):\n",
    "        n = random.choice([1,2,3,4,5,6,7,8,9])\n",
    "        filename = filename + '_' + str(n)\n",
    "    filename = filename + '_rep0.pkl'\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExponentialDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecay_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdecay_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstaircase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A LearningRateSchedule that uses an exponential decay schedule.\n",
      "\n",
      "When training a model, it is often recommended to lower the learning rate as\n",
      "the training progresses. This schedule applies an exponential decay function\n",
      "to an optimizer step, given a provided initial learning rate.\n",
      "\n",
      "The schedule a 1-arg callable that produces a decayed learning\n",
      "rate when passed the current optimizer step. This can be useful for changing\n",
      "the learning rate value across different invocations of optimizer functions.\n",
      "It is computed as:\n",
      "\n",
      "```python\n",
      "def decayed_learning_rate(step):\n",
      "  return initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
      "```\n",
      "\n",
      "If the argument `staircase` is `True`, then `step / decay_steps` is\n",
      "an integer division and the decayed learning rate follows a\n",
      "staircase function.\n",
      "\n",
      "You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n",
      "as the learning rate.\n",
      "Example: When fitting a Keras model, decay every 100000 steps with a base\n",
      "of 0.96:\n",
      "\n",
      "```python\n",
      "initial_learning_rate = 0.1\n",
      "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
      "    initial_learning_rate,\n",
      "    decay_steps=100000,\n",
      "    decay_rate=0.96,\n",
      "    staircase=True)\n",
      "\n",
      "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
      "              loss='sparse_categorical_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "\n",
      "model.fit(data, labels, epochs=5)\n",
      "```\n",
      "\n",
      "The learning rate schedule is also serializable and deserializable using\n",
      "`tf.keras.optimizers.schedules.serialize` and\n",
      "`tf.keras.optimizers.schedules.deserialize`.\n",
      "\n",
      "Returns:\n",
      "  A 1-arg callable learning rate schedule that takes the current optimizer\n",
      "  step and outputs the decayed learning rate, a scalar `Tensor` of the same\n",
      "  type as `initial_learning_rate`.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Applies exponential decay to the learning rate.\n",
      "\n",
      "Args:\n",
      "  initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
      "    Python number.  The initial learning rate.\n",
      "  decay_steps: A scalar `int32` or `int64` `Tensor` or a Python number.\n",
      "    Must be positive.  See the decay computation above.\n",
      "  decay_rate: A scalar `float32` or `float64` `Tensor` or a\n",
      "    Python number.  The decay rate.\n",
      "  staircase: Boolean.  If `True` decay the learning rate at discrete\n",
      "    intervals\n",
      "  name: String.  Optional name of the operation.  Defaults to\n",
      "    'ExponentialDecay'.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "?tf.keras.optimizers.schedules.ExponentialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e372816e4ed8f4776be7691c42762f9e24664f8c81a636e19b7b5da78d6a9ebf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
